# epoch, train loss, evaluation loss
1.0,2023.9157255118234,1845.3993959263394
2.0,1766.4313954598563,1722.7922669328962
3.0,1710.6923085457938,1697.0652402169364
4.0,1690.38508669172,1685.7224982561384
5.0,1678.292565285819,1675.4507891845703
6.0,1671.6411214991979,1672.7510609654018
7.0,1664.0467988368443,1662.7787221854073
8.0,1659.6597368294852,1661.001336931501
9.0,1653.6518891906737,1646.4904374476841
10.0,1642.8636188398089,1639.2238969203404
11.0,1634.9222889055525,1637.8662237548829
12.0,1634.4359982517788,1634.6025742885045
13.0,1631.404176526751,1630.4029158238002
14.0,1628.965222342355,1626.5703260149276
15.0,1626.9630141993932,1621.758785923549
16.0,1623.672693285261,1620.4393237304687
17.0,1624.1068024117606,1615.580379638672
18.0,1621.3550818089077,1617.5900731549943
19.0,1616.3231111580985,1622.9616773332868
20.0,1621.5858777073452,1624.9010900878907
21.0,1619.4131430489676,1613.9928976004464
22.0,1616.1877457972935,1614.6522064208984
23.0,1614.009941558838,1607.1811840820312
24.0,1615.6057988848006,1619.9060214669364
25.0,1614.2354995291573,1621.8099200439453
26.0,1616.31402117048,1617.6445908900669
27.0,1611.2708088030133,1616.2837807791573
28.0,1610.1244478498186,1617.960621948242
29.0,1605.2352917698452,1617.1173575265068
30.0,1609.4349246869767,1616.588542829241
